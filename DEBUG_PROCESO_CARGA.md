# ğŸ” LocalLoom - Debug Completo del Proceso de Carga

## ğŸ“‹ **Proceso Real de Carga de WebLLM**

### ğŸ¯ **Pasos que WebLLM sigue internamente:**

1. **VerificaciÃ³n del modelo**: Buscar en el registro interno de modelos
2. **Descarga de metadatos**: config.json, tokenizer.json 
3. **Descarga de pesos**: archivos .wasm/.bin (600MB para TinyLlama)
4. **CompilaciÃ³n**: OptimizaciÃ³n para WebGPU/WASM
5. **Carga en memoria**: InicializaciÃ³n del modelo

### ğŸ”§ **Comando de Debug Completo**

**Ejecuta esto en Chrome Console (F12) mientras cargas el modelo:**

```javascript
// MONITOR COMPLETO DE CARGA
async function debugCompletoLocalLoom() {
    console.log('ğŸ” === DEBUG COMPLETO LOCALLOOM ===');
    
    // 1. Estado inicial
    console.log('ğŸš€ 1. VERIFICACIÃ“N INICIAL');
    console.log('Chrome:', navigator.userAgent.match(/Chrome\/(\d+)/)?.[1]);
    console.log('WebGPU:', !!navigator.gpu);
    console.log('WASM:', !!WebAssembly);
    
    // 2. Test WebLLM disponible
    console.log('\nğŸ“¦ 2. VERIFICANDO WEBLLM');
    try {
        // Verificar si estÃ¡ disponible globalmente
        if (typeof webllm !== 'undefined') {
            console.log('âœ… WebLLM global disponible');
        } else {
            console.log('âŒ WebLLM global NO disponible');
        }
        
        // Intentar importar dinÃ¡micamente
        const webllmModule = await import('https://esm.run/@mlc-ai/web-llm');
        console.log('âœ… WebLLM importado dinÃ¡micamente:', webllmModule);
        
        // Test crear cliente
        const testClient = new webllmModule.ChatWorkerClient();
        console.log('âœ… ChatWorkerClient creado:', testClient);
        
        // Verificar mÃ©todos disponibles
        console.log('ğŸ¯ MÃ©todos disponibles:', {
            hasReload: typeof testClient.reload === 'function',
            hasChat: !!testClient.chat,
            hasCompletions: !!testClient.chat?.completions,
            hasCreate: !!testClient.chat?.completions?.create
        });
        
    } catch (error) {
        console.error('âŒ Error con WebLLM:', error);
    }
    
    // 3. Monitor de red
    console.log('\nğŸŒ 3. MONITOREANDO RED');
    const originalFetch = window.fetch;
    window.fetch = function(...args) {
        const url = args[0];
        if (typeof url === 'string' && url.includes('mlc')) {
            console.log('ğŸ“¥ WebLLM descarga:', url);
        }
        return originalFetch.apply(this, args);
    };
    
    // 4. Monitor del service worker
    console.log('\nğŸ› ï¸ 4. MONITOREANDO SERVICE WORKER');
    navigator.serviceWorker.addEventListener('message', (event) => {
        if (event.data.type && event.data.type.includes('MODEL')) {
            console.log('ğŸ“¨ SW Message:', event.data);
        }
    });
    
    console.log('\nâœ… Debug configurado. Ahora intenta cargar el modelo.');
    console.log('ğŸ“Š VerÃ¡s todos los logs aquÃ­ automÃ¡ticamente.');
}

// Ejecutar debug
debugCompletoLocalLoom();
```

### ğŸ¯ **Test Manual del Modelo**

**Para verificar si el modelo se puede cargar directamente:**

```javascript
// TEST DIRECTO DE WEBLLM
async function testDirectoWebLLM() {
    console.log('ğŸ§ª === TEST DIRECTO WEBLLM ===');
    
    try {
        // 1. Importar WebLLM
        const webllm = await import('https://esm.run/@mlc-ai/web-llm');
        console.log('âœ… WebLLM importado');
        
        // 2. Crear engine
        const engine = new webllm.ChatWorkerClient();
        console.log('âœ… Engine creado');
        
        // 3. Configurar callback
        const progressCallback = (progress) => {
            console.log(`â³ ${Math.round(progress.progress * 100)}% - ${progress.text}`);
        };
        
        // 4. Intentar cargar TinyLlama
        console.log('ğŸ“¥ Cargando TinyLlama...');
        console.time('â±ï¸ Tiempo de carga');
        
        await engine.reload('TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC', undefined, {
            progressCallback: progressCallback
        });
        
        console.timeEnd('â±ï¸ Tiempo de carga');
        console.log('âœ… MODELO CARGADO EXITOSAMENTE');
        
        // 5. Test de generaciÃ³n
        console.log('ğŸ§  Probando generaciÃ³n...');
        const response = await engine.chat.completions.create({
            messages: [{ role: 'user', content: 'Hola, Â¿funciona?' }],
            max_tokens: 50
        });
        
        console.log('ğŸ¯ Respuesta:', response.choices[0].message.content);
        console.log('âœ… TODO FUNCIONA PERFECTAMENTE');
        
    } catch (error) {
        console.error('âŒ ERROR:', error);
        console.error('ğŸ” Detalles:', error.message);
        console.error('ğŸ“š Stack:', error.stack);
    }
}

// Ejecutar test directo
testDirectoWebLLM();
```

## ğŸ” **AnÃ¡lisis de Problemas Comunes**

### Problem 1: "Failed to fetch"
```
âŒ Indica: Error de red durante descarga
ğŸ”§ SoluciÃ³n: Verificar conexiÃ³n, firewall, antivirus
```

### Problem 2: "Model not found"
```
âŒ Indica: ID de modelo incorrecto
ğŸ”§ SoluciÃ³n: Verificar que sea exactamente 'TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC'
```

### Problem 3: "Out of memory"
```
âŒ Indica: RAM insuficiente
ğŸ”§ SoluciÃ³n: Cerrar otras pestaÃ±as, necesitas ~2GB libre
```

### Problem 4: "WebGPU not supported"
```
âŒ Indica: GPU no compatible
ğŸ”§ SoluciÃ³n: Funciona igual con WASM (mÃ¡s lento)
```

### Problem 5: "Service Worker error"
```
âŒ Indica: Error en la extensiÃ³n
ğŸ”§ SoluciÃ³n: Recargar extensiÃ³n en chrome://extensions
```

## ğŸ® **Estados del Proceso**

### âœ… **Estado 1: Iniciando**
```
ğŸš€ Iniciando carga de modelo WebLLM: TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC
ğŸ”§ Creando ChatWorkerClient...
âœ… ChatWorkerClient creado exitosamente
```

### âœ… **Estado 2: Descargando**
```
ğŸ“Š Progreso WebLLM: {progress: 0.1, text: "Downloading..."}
â³ Progreso: 10% - Downloading...
ğŸ“¦ Descargado: 60.0MB / 600.0MB
```

### âœ… **Estado 3: Compilando** 
```
ğŸ“Š Progreso WebLLM: {progress: 0.8, text: "Compiling..."}
â³ Progreso: 80% - Compiling...
```

### âœ… **Estado 4: Completado**
```
âœ… Modelo cargado exitosamente: TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC
ğŸ¯ API disponible: {hasChat: true, hasCompletions: true, hasCreate: true}
```

## ğŸ“ **Si nada funciona**

1. **Prueba el test directo** - Si falla, es problema de WebLLM/red
2. **Verifica en otra pestaÃ±a** - Abre https://chat.webllm.ai/ 
3. **Revisa el service worker** - Pueden estar muriendo
4. **Intenta otro modelo** - Prueba 'Qwen2.5-0.5B-Instruct-q4f16_1-MLC' (mÃ¡s pequeÃ±o)

---

**ğŸ” El debug te dirÃ¡ exactamente dÃ³nde falla el proceso.** 